{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNvA5zAWa47lJEVfUmoZH9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinRaikar88/dive2dl/blob/tensorflow/2.1_Data_Manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nEkvd0Jumitk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Data Manipulation \n",
        "\n",
        "A tensor represents a (possibly multi-dimensional) array of numerical values. \n",
        "\n",
        "\n",
        "* With *one axis* (K=1), a tensor is called a **Vector**. \n",
        "\n",
        "* With *two axes* (K=2), a tensor is called a **Matrix**. \n",
        "\n",
        "* With *K > 2 axes*, we drop the specialized names and just refer to the object as a **Kth order tensor**."
      ],
      "metadata": {
        "id": "aPpeKwKPm40-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.range()\n",
        "\n",
        "The range(n) function, creates a vector of evenly spaced values, starting at 0 (included) and ending at n (not included). By Default the step size (Delta) is 1.Unless otherwise specified, New tensors are stored in main memory and designated for CPU-based computation."
      ],
      "metadata": {
        "id": "4dQE6_JPnzA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.range(12, dtype=tf.float32)\n",
        "x # x is a vector with 12 elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZnrGlQWmkEQ",
        "outputId": "6128c12e-1729-4673-b6c5-344aac15fb05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
              "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.range(12, delta=2, dtype=tf.float32)\n",
        "x # we will have 6 elements (Note that 12 is excluded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1tALJH3pdnz",
        "outputId": "ced6299c-d9ee-4276-ed6b-9cc89047f220"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([ 0.,  2.,  4.,  6.,  8., 10.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.range(start=1, limit=13, delta=1, dtype=tf.float32)\n",
        "x # we will have 12 elements starting from 1 to 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdfPE-Gvpdvk",
        "outputId": "58b7302e-8e7a-4af9-adba-2fe35ca73828"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
              "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of these values is called an element of the tensor. The tensor x contains 12 elements. We can inspect the total number of elements in a tensor via the size function."
      ],
      "metadata": {
        "id": "OjGkfKr2oldF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-oj617kmp33",
        "outputId": "f44a3f9c-3771-4872-ddcf-0b1480a7a6c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([12])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.reshape()\n",
        "\n",
        "We can change the shape of a tensor without altering its size or values, by invoking reshape. For example, we can transform our vector x whose shape is (12,) to a matrix X with shape (3, 4). This new tensor retains all elements but reconfigures them into a matrix. Notice that the elements of our vector are laid out one row at a time and thus x[3] == X[0, 3]."
      ],
      "metadata": {
        "id": "L_-hjCuco7Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.reshape(x, (3, 4))\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyvLetQEowQs",
        "outputId": "e0b9afa3-5dbd-4954-f7f8-83ceb89cbd87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 1.,  2.,  3.,  4.],\n",
              "       [ 5.,  6.,  7.,  8.],\n",
              "       [ 9., 10., 11., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that the elements of our vector are laid out one row at a time and thus x[3] == X[0, 3]\n",
        "x[3] == X[0][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGDSo1QFpG03",
        "outputId": "d9b1695e-99ad-428a-e498-32c5cced3e10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that specifying every shape component to reshape is redundant. Because we already know our tensorâ€™s size, we can work out one component of the shape given the rest. \n",
        "\n",
        "For example, given a tensor of size  and target shape (, ), we know that . To automatically infer one component of the shape, we can place a -1 for the shape component that should be inferred automatically. In our case, instead of calling x.reshape(3, 4), we could have equivalently called x.reshape(-1, 4) or x.reshape(3, -1)."
      ],
      "metadata": {
        "id": "-hgWiOCEsY8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(3, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnAKwf48sYjz",
        "outputId": "acd5d19f-483c-483c-973f-ad089668f84c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 1.,  2.,  3.,  4.],\n",
              "       [ 5.,  6.,  7.,  8.],\n",
              "       [ 9., 10., 11., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(-1, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM6cG-7Lr34H",
        "outputId": "cd93d47f-8ca3-4e0b-ca69-795b43534a36"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 1.,  2.,  3.,  4.],\n",
              "       [ 5.,  6.,  7.,  8.],\n",
              "       [ 9., 10., 11., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.zeros\n",
        "\n",
        "We can construct a tensor with all elements set to zero and a shape of (2, 3, 4) via the zeros function.\n"
      ],
      "metadata": {
        "id": "BR5hpLo-JVSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.zeros((2, 3, 4))"
      ],
      "metadata": {
        "id": "IvtH-4PEvHKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0fa6b4-6519-43bf-ecdc-79c484a58f72"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
              "array([[[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.ones()\n",
        "\n",
        "Similarly, we can create a tensor with all ones by invoking ones."
      ],
      "metadata": {
        "id": "LQ5RL9QgJsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.ones((2, 3, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA-3YQG2JeXV",
        "outputId": "93855bba-b3d2-47e6-dcfe-47e8c3a341c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
              "array([[[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]],\n",
              "\n",
              "       [[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.random.normal()\n",
        "\n",
        "The following snippet creates a tensor with elements drawn from a standard Gaussian (normal) distribution with mean 0 and standard deviation 1."
      ],
      "metadata": {
        "id": "7Fa0ea8dJ-Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.normal(shape=[3, 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6YdoHCGJ99A",
        "outputId": "bb28949b-bb3d-472c-c960-0a7b92f94b2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 1.4689566 , -0.6965561 , -0.42607895, -0.5248851 ],\n",
              "       [ 0.6930743 , -1.5239799 , -1.2153347 , -1.2565767 ],\n",
              "       [ 0.2819825 ,  1.525284  ,  2.3586066 ,  0.12328853]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.constant()\n",
        "\n",
        "Finally, we can construct tensors by supplying the exact values for each element by supplying (possibly nested) Python list(s) containing numerical literals. Here, we construct a matrix with a list of lists, where the outermost list corresponds to axis 0, and the inner list to axis 1."
      ],
      "metadata": {
        "id": "-_hmTjN3KTqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLmPrMDpJxHu",
        "outputId": "1d3e9a4b-99b7-4f73-e972-3cdc19b26e8a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
              "array([[2, 1, 4, 3],\n",
              "       [1, 2, 3, 4],\n",
              "       [4, 3, 2, 1]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.2 Indexing and Slicing\n",
        "\n",
        "As with Python lists, we can access tensor elements by indexing (starting with 0). To access an element based on its position relative to the end of the list, we can use negative indexing. Finally, we can access whole ranges of indices via slicing (e.g., X[start:stop]), where the returned value includes the first index (start) but not the last (stop). Finally, when only one index (or slice) is specified for a Kth order tensor, it is applied along axis 0. Thus, in the following code, [-1] selects the last row and [1:3] selects the second and third rows."
      ],
      "metadata": {
        "id": "4wZ2Uis3KrjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mxJT8VQLTAg",
        "outputId": "431cebbf-9ccd-4d15-b4d2-82644957a668"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 1.,  2.,  3.,  4.],\n",
              "       [ 5.,  6.,  7.,  8.],\n",
              "       [ 9., 10., 11., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access Rows\n",
        "X[-1], X[1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSyy8p-KaLQ",
        "outputId": "572b347a-74b7-4b95-8386-3ce19246a515"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 9., 10., 11., 12.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
              " array([[ 5.,  6.,  7.,  8.],\n",
              "        [ 9., 10., 11., 12.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access Cols\n",
        "for col_index in range(4):\n",
        "  for i in X:    \n",
        "    print(i[col_index])\n",
        "  print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xhJJFNQM4pW",
        "outputId": "3080b690-375a-4866-bc4d-3548c3d22d17"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "tf.Tensor(5.0, shape=(), dtype=float32)\n",
            "tf.Tensor(9.0, shape=(), dtype=float32)\n",
            "------\n",
            "tf.Tensor(2.0, shape=(), dtype=float32)\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "tf.Tensor(10.0, shape=(), dtype=float32)\n",
            "------\n",
            "tf.Tensor(3.0, shape=(), dtype=float32)\n",
            "tf.Tensor(7.0, shape=(), dtype=float32)\n",
            "tf.Tensor(11.0, shape=(), dtype=float32)\n",
            "------\n",
            "tf.Tensor(4.0, shape=(), dtype=float32)\n",
            "tf.Tensor(8.0, shape=(), dtype=float32)\n",
            "tf.Tensor(12.0, shape=(), dtype=float32)\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors in TensorFlow are immutable, and cannot be assigned to. Variables in TensorFlow are mutable containers of state that support assignments. Keep in mind that gradients in TensorFlow do not flow backwards through Variable assignments.\n",
        "\n",
        "Beyond assigning a value to the entire Variable, we can write elements of a Variable by specifying indices."
      ],
      "metadata": {
        "id": "Z4HNAdGYMmaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables"
      ],
      "metadata": {
        "id": "_cXlpITSOaWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_var = tf.Variable(X)\n",
        "X_var[1, 2].assign(9)\n",
        "X_var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y_HczoQLM4X",
        "outputId": "74bfa9ff-e36a-4f71-c01a-029b52973d2b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
              "array([[ 1.,  2.,  3.,  4.],\n",
              "       [ 5.,  6.,  9.,  8.],\n",
              "       [ 9., 10., 11., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to assign multiple elements the same value, we apply the indexing on the left-hand side of the assignment operation. For instance, [:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this also works for vectors and for tensors of more than 2 dimensions."
      ],
      "metadata": {
        "id": "h55JediYOzkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_var = tf.Variable(X)\n",
        "X_var[:2, :].assign(tf.ones(X_var[:2,:].shape, dtype=tf.float32) * 12)\n",
        "X_var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q51eSd8MLf6J",
        "outputId": "e5492df3-b610-47c0-8d75-839d72149933"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
              "array([[12., 12., 12., 12.],\n",
              "       [12., 12., 12., 12.],\n",
              "       [12., 12., 12., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_var = tf.Variable(X)\n",
        "X_var[:,:].assign(tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype='float32') * 12)\n",
        "X_var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epLtcIGjPYq5",
        "outputId": "e3a2fc78-7bda-48d9-fe00-905f2617f98d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
              "array([[24., 12., 48., 36.],\n",
              "       [12., 24., 36., 48.],\n",
              "       [48., 36., 24., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.3 Operations\n",
        "\n",
        "The Elementwise operations apply a standard scalar operation to each element of a tensor. For functions that take two tensors as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements. We can create an elementwise function from any function that maps from a scalar to a scalar.\n"
      ],
      "metadata": {
        "id": "SCdTUxvXSBdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.exp()"
      ],
      "metadata": {
        "id": "5LnY4d36SbR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6djZH1zQGwi",
        "outputId": "095f8c7d-edd5-4225-ace3-e94b5024290d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
              "array([2.7182817e+00, 7.3890562e+00, 2.0085537e+01, 5.4598148e+01,\n",
              "       1.4841316e+02, 4.0342877e+02, 1.0966332e+03, 2.9809580e+03,\n",
              "       8.1030840e+03, 2.2026465e+04, 5.9874141e+04, 1.6275480e+05],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The common standard arithmetic operators for addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**) have all been lifted to elementwise operations for identically-shaped tensors of arbitrary shape."
      ],
      "metadata": {
        "id": "kQZlqQ9mSom4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([1.0, 2, 4, 8])\n",
        "y = tf.constant([2.0, 2, 2, 2])\n",
        "x + y, x - y, x * y, x / y, x ** y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYk5nJ8UShCo",
        "outputId": "087608fe-f250-4270-8d81-38b56438ee8f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)>,\n",
              " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.concat()\n",
        "\n",
        "We can also concatenate multiple tensors together, stacking them end-to-end to form a larger tensor. We just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate two matrices along rows (axis 0) vs. columns (axis 1). We can see that the first outputâ€™s axis-0 length (6) is the sum of the two input tensorsâ€™ axis-0 lengths (3+3); while the second outputâ€™s axis-1 length (8) is the sum of the two input tensorsâ€™ axis-1 lengths (4+4)."
      ],
      "metadata": {
        "id": "wrokjRZJTRLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))\n",
        "Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sON1PzhYSsP1",
        "outputId": "297c6f0e-4f60-48b0-ad41-66495125e047"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(6, 4), dtype=float32, numpy=\n",
              " array([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.],\n",
              "        [ 2.,  1.,  4.,  3.],\n",
              "        [ 1.,  2.,  3.,  4.],\n",
              "        [ 4.,  3.,  2.,  1.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
              " array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example. For each position i, j, if X[i, j] and Y[i, j] are equal, then the corresponding entry in the result takes value 1, otherwise it takes value 0."
      ],
      "metadata": {
        "id": "ZQDlSJJqT2OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X == Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8QL4YDETWso",
        "outputId": "52b6cf0e-fa0c-4e98-a395-b49c056b5f0b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
              "array([[False,  True, False,  True],\n",
              "       [False, False, False, False],\n",
              "       [False, False, False, False]])>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.reduce_sum()\n",
        "\n",
        "Summing all the elements in the tensor yields a tensor with only one element."
      ],
      "metadata": {
        "id": "y9gdf4EnT8TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_sum(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9O9ErMmT36a",
        "outputId": "1655bb2c-1226-4b9a-9068-313aa8878d71"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=66.0>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.4. Broadcasting\n",
        "\n",
        "By now, you know how to perform elementwise binary operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise binary operations by invoking the broadcasting mechanism. \n",
        "\n",
        "\n",
        "Broadcasting works according to the following two-step procedure: \n",
        "\n",
        "1. Expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape\n",
        "\n",
        "2. Perform an elementwise operation on the resulting arrays."
      ],
      "metadata": {
        "id": "E5ObzO9-cRxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.reshape(tf.range(3), (3, 1))\n",
        "b = tf.reshape(tf.range(2), (1, 2))\n",
        "a, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDOpm1Kqck35",
        "outputId": "f0de9332-dc18-452b-aed2-761ed8c57cd0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
              " array([[0],\n",
              "        [1],\n",
              "        [2]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since a and b are 3 x 1 and 1 x 2 matrices, respectively, their shapes do not match up. Broadcasting produces a larger 3 x 2 matrix by replicating matrix **a** along the columns and matrix **b** along the rows before adding them elementwise."
      ],
      "metadata": {
        "id": "kO-8Dntzcozg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt9TGuMAc5C5",
        "outputId": "2833e11b-8fd7-487f-89e2-4cf9d18d082b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
              "array([[0, 1],\n",
              "       [1, 2],\n",
              "       [2, 3]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.5 Saving Memory\n",
        "\n",
        "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. We can demonstrate this issue with Pythonâ€™s id() function, which gives us the exact address of the referenced object in memory. Note that after we run Y = Y + X, id(Y) points to a different location. Thatâ€™s because Python first evaluates Y + X, allocating new memory for the result and then points Y to this new location in memory.\n"
      ],
      "metadata": {
        "id": "0bK5YEFpX-R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(Y)\n",
        "Y = Y + X\n",
        "id(Y) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiFaxJC3UAQz",
        "outputId": "4177a498-f40c-405b-accd-82e6f853b3a4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we often have hundreds of megabytes of parameters and update all of them multiple times per second. Whenever possible, we want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, we must be careful to update all of these references, lest we spring a memory leak or inadvertently refer to stale parameters.\n",
        "\n",
        "Variables are mutable containers of state in TensorFlow. They provide a way to store your model parameters. We can assign the result of an operation to a Variable with assign. To illustrate this concept, we overwrite the values of Variable Z after initializing it, using zeros_like, to have the same shape as Y."
      ],
      "metadata": {
        "id": "q_J1NqaEYJm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = tf.Variable(tf.zeros_like(Y))\n",
        "print('id(Z):', id(Z))\n",
        "Z.assign(X + Y)\n",
        "print('id(Z):', id(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfxrxDMmYF9A",
        "outputId": "ad7808b5-e765-4793-89db-08b06b2cac43"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id(Z): 140147407691728\n",
            "id(Z): 140147407691728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.fucntion()\n",
        "\n",
        "Even once you store state persistently in a Variable, you may want to reduce your memory usage further by avoiding excess allocations for tensors that are not your model parameters. Because TensorFlow Tensors are immutable and gradients do not flow through Variable assignments, TensorFlow does not provide an explicit way to run an individual operation in-place.\n",
        "\n",
        "However, TensorFlow provides the tf.function decorator to wrap computation inside of a TensorFlow graph that gets compiled and optimized before running. This allows TensorFlow to prune unused values, and to reuse prior allocations that are no longer needed. This minimizes the memory overhead of TensorFlow computations."
      ],
      "metadata": {
        "id": "EnI01BGtYaWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def computation(X, Y):\n",
        "    Z = tf.zeros_like(Y)  # This unused value will be pruned out\n",
        "    A = X + Y  # Allocations will be reused when no longer needed\n",
        "    B = A + Y\n",
        "    C = B + Y\n",
        "    return C + Y\n",
        "\n",
        "computation(X, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFzv2unTYOia",
        "outputId": "16b1c801-48f8-4b9c-812d-f51ba4ab22c1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[ 8.,  9., 26., 27.],\n",
              "       [24., 33., 42., 51.],\n",
              "       [56., 57., 58., 59.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.6. Conversion to Other Python Objects\n",
        "\n",
        "Converting to a NumPy tensor (ndarray), or vice versa, is easy. The converted result does not share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or on GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory."
      ],
      "metadata": {
        "id": "J99qwWmKZFad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = X.numpy()\n",
        "B = tf.constant(A)\n",
        "type(A), type(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27JRCpTBYs6k",
        "outputId": "61fb724c-2c3d-493d-eb9e-ed4f28eb015f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert a size-1 tensor to a Python scalar, we can invoke the item function or Pythonâ€™s built-in functions."
      ],
      "metadata": {
        "id": "H5osoqjZZt7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([3.5]).numpy()\n",
        "a, a.item(), float(a), int(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O3jmgWLZmmC",
        "outputId": "d8ca6a8a-55f3-4719-bd3a-d21bc1b5680a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.5], dtype=float32), 3.5, 3.5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.7. Summary\n",
        "\n",
        "The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects."
      ],
      "metadata": {
        "id": "57y6X2Rrav8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.8. Exercises\n"
      ],
      "metadata": {
        "id": "7tzFB4TAa8Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the conditional statement X == Y to X < Y or X > Y, and then see what kind of tensor you can get."
      ],
      "metadata": {
        "id": "m2GeEJmGbCwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X == Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRQDSknVb7FI",
        "outputId": "924ca735-2a4a-4780-a50b-8309c3ea81ce"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
              "array([[False,  True, False,  True],\n",
              "       [False, False, False, False],\n",
              "       [False, False, False, False]])>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X != Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81yT-GrlZxa0",
        "outputId": "b86211a7-e514-4bb7-8693-0de7168f72b6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
              "array([[ True, False,  True, False],\n",
              "       [ True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True]])>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X < Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzjTx--LbZX2",
        "outputId": "51f15ede-5693-4254-a914-b2c8d69ff8c5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
              "array([[ True, False,  True, False],\n",
              "       [False, False, False, False],\n",
              "       [False, False, False, False]])>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X > Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGS6Sfq9bZfx",
        "outputId": "5b892dd3-5415-4380-ec81-dc07860d124a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
              "array([[False, False, False, False],\n",
              "       [ True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True]])>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?"
      ],
      "metadata": {
        "id": "0u-NtzVfbC47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.reshape(tf.range(12), (3, 1, 4))\n",
        "b = tf.reshape(tf.range(2), (1, 2 , 1))\n",
        "a, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBYeYrZ3bAv1",
        "outputId": "688dd811-a370-4a02-dec0-892a816cb1ad"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(3, 1, 4), dtype=int32, numpy=\n",
              " array([[[ 0,  1,  2,  3]],\n",
              " \n",
              "        [[ 4,  5,  6,  7]],\n",
              " \n",
              "        [[ 8,  9, 10, 11]]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(1, 2, 1), dtype=int32, numpy=\n",
              " array([[[0],\n",
              "         [1]]], dtype=int32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WIHEG-4dCO3",
        "outputId": "531da55a-b505-4bf0-c5a8-f9e48ff970cf"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 4), dtype=int32, numpy=\n",
              "array([[[ 0,  1,  2,  3],\n",
              "        [ 1,  2,  3,  4]],\n",
              "\n",
              "       [[ 4,  5,  6,  7],\n",
              "        [ 5,  6,  7,  8]],\n",
              "\n",
              "       [[ 8,  9, 10, 11],\n",
              "        [ 9, 10, 11, 12]]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSyqRCDAdIYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}